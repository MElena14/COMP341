{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MElena14/COMP341/blob/main/assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfd5JgWoICcg"
      },
      "source": [
        "#### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k3dxF3QaJqeD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/sgmsempl/.conda/envs/torch/lib/python3.9/site-packages/ale_py/roms/__init__.py:89: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
            "  ROMS = resolve_roms()\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import math\n",
        "from collections import deque, namedtuple\n",
        "import cv2\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm\n",
        "import PIL\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyBEYPP0H-qS"
      },
      "source": [
        "#### Render OpenAI Gym Environments from CoLab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "AL2uvFBoH4ji",
        "outputId": "4238a76d-4f8d-48ff-c34b-b1af70f9ff7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "not in colab\n",
            "Using device cuda:1\n"
          ]
        }
      ],
      "source": [
        "# HIDE OUTPUT\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
        "    env_name = \"Atlantis-v0\"\n",
        "    !wget http://www.atarimania.com/roms/Roms.rar \n",
        "    !unrar x -o+ /content/Roms.rar >/dev/nul\n",
        "    !python -m atari_py.import_roms /content/ROMS >/dev/nul\n",
        "\n",
        "    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "    !apt-get update > /dev/null 2>&1\n",
        "    !apt-get install cmake > /dev/null 2>&1\n",
        "    !pip install --upgrade setuptools 2>&1\n",
        "    !pip install ez_setup > /dev/null 2>&1\n",
        "    !pip install gym[atari] > /dev/null 2>&1\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    env_name = \"ALE/Atlantis-v5\"\n",
        "    print('not in colab')\n",
        "    device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:1\")\n",
        "print(\"Using device\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjkT6v3pHnVv",
        "outputId": "5927ff8d-d5de-46f3-f192-70708746f48f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
            "[Powered by Stella]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action Space: Discrete(4)\n",
            "possible actions: ['NOOP', 'FIRE', 'RIGHTFIRE', 'LEFTFIRE']\n",
            "Observation Space: Box(0, 255, (210, 160, 3), uint8)\n",
            "Max Episode Steps: 27000\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: None\n"
          ]
        }
      ],
      "source": [
        "def query_environment(name):\n",
        "  env = gym.make(name)\n",
        "  spec = gym.spec(name)\n",
        "  print(f\"Action Space: {env.action_space}\")\n",
        "  print(f\"possible actions: {env.unwrapped.get_action_meanings()}\")\n",
        "  print(f\"Observation Space: {env.observation_space}\")\n",
        "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "  print(f\"Reward Range: {env.reward_range}\")\n",
        "  print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
        "  \n",
        "query_environment(env_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8X-yeVrRHpTf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# display = Display(visible=0, size=(1400, 900))\n",
        "# display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment \n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  print(mp4list[-1])\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[-1]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "if IN_COLAB:\n",
        "    from gym.wrappers import Monitor\n",
        "    def wrap_env(env, record_every=1, folder='./video'):\n",
        "        env = Monitor(env, folder, force=True)\n",
        "        return env\n",
        "else:\n",
        "    from gym.wrappers.record_video import RecordVideo\n",
        "    def wrap_env(env, record_every=1, folder='./video'):\n",
        "        env = RecordVideo(env, folder, episode_trigger=lambda i: i % record_every == 0)\n",
        "        return env\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvpErA1hITFs"
      },
      "source": [
        "#### inital random agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AVzkJJHendfi"
      },
      "outputs": [],
      "source": [
        "def show_random_agent():\n",
        "  env = wrap_env(gym.make(env_name))\n",
        "\n",
        "  observation = env.reset()\n",
        "  score = 0\n",
        "  while True:\n",
        "      env.render()\n",
        "      action = env.action_space.sample()\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      score += reward\n",
        "      if done:\n",
        "        print(f\"finished! random agent's score is {score}\")\n",
        "        break\n",
        "  env.close()\n",
        "  show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkV2DFe4JwC9"
      },
      "source": [
        "#### Deep QN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Mx-ZVJ4s4nV_"
      },
      "outputs": [],
      "source": [
        "class DQNCnn(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super().__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "        \n",
        "        self.features = nn.Sequential( # in = 84x84\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        ) # out = 7x7\n",
        "        self.feature_size = 7 * 7 * 64\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, self.num_actions)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg5IwV9Gndfl"
      },
      "source": [
        "#### ResNetDQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krpJGCQ-ndfl"
      },
      "source": [
        "##### ResNetBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SsP2JnGWndfm"
      },
      "outputs": [],
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, depth_in, activation_func, depth_out=-1):\n",
        "        super().__init__()\n",
        "        self.downSampleResidul = True\n",
        "        if depth_out == -1:\n",
        "            depth_out = depth_in\n",
        "            self.downSampleResidul = False\n",
        "\n",
        "        self.resBlock = nn.Sequential(\n",
        "            nn.BatchNorm2d(depth_in),\n",
        "            activation_func(inplace=True),\n",
        "            nn.Conv2d(depth_in, depth_out, kernel_size=3, padding=1, stride=1 if not self.downSampleResidul else 2, bias=False),\n",
        "            nn.BatchNorm2d(depth_out),\n",
        "            activation_func(inplace=True),\n",
        "            nn.Conv2d(depth_out, depth_out, kernel_size=3, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.downsampleRes = nn.Sequential(\n",
        "            nn.BatchNorm2d(depth_in),\n",
        "            activation_func(inplace=True),\n",
        "            nn.Conv2d(depth_in, depth_out, kernel_size=1, stride=2, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.resBlock(x)\n",
        "        if self.downSampleResidul:\n",
        "            x = self.downsampleRes(x)\n",
        "        return z + x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPg5kxuundfn",
        "outputId": "2edfdf27-7973-4d6c-8645-fc33d9cf2687"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "not int 9.5\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_cnn_output_size(in_size, kernel_size, stride=1, padding=0):\n",
        "    integer = int((in_size - kernel_size + 2 * padding) / stride) == ((in_size - kernel_size + 2 * padding) / stride)\n",
        "    if not integer:\n",
        "        print(\"not int\", ((in_size - kernel_size + 2 * padding) / stride))\n",
        "    return int((in_size - kernel_size + 2 * padding) / stride) + 1\n",
        "get_cnn_output_size(in_size=20, kernel_size=3, stride=2, padding=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTcVbQMrndfo"
      },
      "source": [
        "##### ResNetDQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9zx6rGfDndfp"
      },
      "outputs": [],
      "source": [
        "class ResNetDQN(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "        super(ResNetDQN, self).__init__()\n",
        "        #in = 159x159\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 16, kernel_size=3, stride=2, padding=0), #out = 79x79\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=0), #out = 77x77\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0), #out = 75x75\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0) #out = 37x37\n",
        "        )\n",
        "        \n",
        "        self.featureExtractionBlock = nn.Sequential(\n",
        "            ResNetBlock(32, nn.SiLU, 64),  #out = 19x19\n",
        "            ResNetBlock(64, nn.SiLU),      #out = 19x19\n",
        "            ResNetBlock(64, nn.SiLU),      #out = 19x19 \n",
        "            ResNetBlock(64, nn.SiLU, 128), #out = 10x10\n",
        "            ResNetBlock(128, nn.SiLU),     #out = 10x10\n",
        "        )\n",
        "        # self.pool = nn.AdaptiveAvgPool2d((4,4))\n",
        "        self.flattened_size = 128 * 10 * 10\n",
        "        self.regressionBlock = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(self.flattened_size, 1024),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Linear(1024, self.num_actions),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.featureExtractionBlock(x)\n",
        "        # x = self.pool(x)\n",
        "        x = x.view(-1, self.flattened_size)\n",
        "        x = self.regressionBlock(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSFkzLbg0p7q"
      },
      "source": [
        "#### utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "uS4Ms1_30tbT",
        "outputId": "3dd6f4ca-7ad1-4481-d2b0-0a4f33a5262c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJ8AAACfCAAAAADCcFQ8AAABcElEQVR4nO3YQSuDARzH8Z/t2TzMeswew5ikyWEUtoakXFZTkkQtZ+XAO/BWvAgXJRe5yQ333cjJxQVrceBGS/zX85Tv5/I8p1/fnqfneeqRAAAAAAAAAAAAAAAAAAC/0RF0QEtOLhJ0QkvdmyG+ftWKIp4TdMVXucLHsRi9n74IYd/UwefJ2fnkUQjvbzQmSdtDajZjLyHs+1BIKj0a7vfL+IIT6j71r4S7zx0MugAAAAD4z+L7cdtB4/8vb3dvUn4iXZZUzBgMGvc1jhvS0vJYTdJW3nbbgL8rabVcWh/ek9yowaLp9csu3mjNq6f8p7lr6blpMGnaN1i8cle9up9+nL90LYfNJE4SOtyQ1HniBd3yHadaSc6O5WYUrcbLAwaDts9vpOs0k7p96PcjidPXgW7TbQt9NWlttFRW707QKd+KDUkZ1/PkZDXfE3RNa4vJoAsA/NRIqh2rdt+PXFv6AAAAAAAAAAAAAAAA/uodiHQsHPCDfL8AAAAASUVORK5CYII=",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=159x159 at 0x7F47A51099D0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# taken from https://github.com/deepanshut041/Reinforcement-Learning/blob/master/algos/preprocessing/stack_frame.py\n",
        "def preprocess_frame(screen, exclude, output):\n",
        "    \"\"\"Preprocess Image.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            screen (array): RGB Image\n",
        "            exclude (tuple): Section to be croped (UP, RIGHT, DOWN, LEFT)\n",
        "            output (int): Size of output image\n",
        "        \"\"\"\n",
        "    # TConver image to gray scale\n",
        "    screen = cv2.cvtColor(screen, cv2.COLOR_RGB2GRAY)\n",
        "    \n",
        "    #Crop screen[Up: Down, Left: right] \n",
        "    screen = screen[exclude[0]:exclude[2], exclude[3]:exclude[1]]\n",
        "    \n",
        "    # Convert to float, and normalized\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    \n",
        "    # Resize image to 84 * 84\n",
        "    screen = cv2.resize(screen, (output, output), interpolation = cv2.INTER_AREA)\n",
        "    return screen\n",
        "\n",
        "def stack_frame(stacked_frames, frame, is_new):\n",
        "    \"\"\"Stacking Frames.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            stacked_frames (array): Four Channel Stacked Frame\n",
        "            frame: Preprocessed Frame to be added\n",
        "            is_new: Is the state First\n",
        "        \"\"\"\n",
        "    if is_new:\n",
        "        stacked_frames = np.stack(arrays=[frame, frame, frame, frame])\n",
        "        stacked_frames = stacked_frames\n",
        "    else:\n",
        "        stacked_frames[0] = stacked_frames[1]\n",
        "        stacked_frames[1] = stacked_frames[2]\n",
        "        stacked_frames[2] = stacked_frames[3]\n",
        "        stacked_frames[3] = frame\n",
        "    \n",
        "    return stacked_frames\n",
        "\n",
        "frame_crops = {'DQNcnn': 84, 'ResNetDQN': 159}\n",
        "\n",
        "def get_stack_frames(net='DQNcnn'):\n",
        "    size = frame_crops[net]\n",
        "    def stack_frames(frames, state, is_new=False):\n",
        "        frame = preprocess_frame(state, (0, -10, -100, 9), size)\n",
        "        frames = stack_frame(frames, frame, is_new)\n",
        "        return frames\n",
        "    return stack_frames\n",
        "\n",
        "def show_cropped_image():\n",
        "    env = gym.make(env_name)\n",
        "    observation = env.reset()\n",
        "    t, done = 0, False\n",
        "    while not done and t < 260:\n",
        "      # env.render()\n",
        "      action = env.action_space.sample() \n",
        "      observation, reward, done, info = env.step(action)\n",
        "      \n",
        "      t += 1\n",
        "                                # (UP, RIGHT, DOWN, LEFT)\n",
        "    f = preprocess_frame(observation, (0, -10, -100, 9), 159)\n",
        "    return PIL.Image.fromarray(np.uint8(f * 255), mode=\"L\")\n",
        "show_cropped_image()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "gQK4tV3qndfu"
      },
      "outputs": [],
      "source": [
        "def plot_epsilon_func(eps_func, n_episodes=1000):\n",
        "    episodes = range(n_episodes)\n",
        "    eps = [eps_func(i) for i in episodes]\n",
        "    plt.plot(episodes, eps)\n",
        "    plt.show()\n",
        "\n",
        "def iqr_mean(scores):\n",
        "    scores = np.array(scores)\n",
        "    q25, q75 = np.percentile(scores, [25, 75])\n",
        "    meaners = scores[np.logical_and(scores > q25, scores < q75)]\n",
        "    return q25, q75, meaners.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtzA_Zt7tTOs"
      },
      "source": [
        "### memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "W14ncSaOKCVj"
      },
      "outputs": [],
      "source": [
        "# Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "from numpy.typing import NDArray\n",
        "class ReplayMemory:\n",
        "    def __init__(self, buffer_size, batch_size, device, filename=None, top=1):\n",
        "        self.memory = deque(maxlen=buffer_size) if filename is None else pickle.load(open(filename, 'rb'))\n",
        "        self.batch_size = int(batch_size/top) if top < 1 else batch_size\n",
        "        self.top = top\n",
        "        self.device = device\n",
        "        self.mean = lambda l: sum(l)/len(l)\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        Experience = (state, action, reward, next_state, float(done))\n",
        "        self.memory.append(Experience)\n",
        "    def _as_tensor(self, np_array:NDArray, dtype=torch.float):\n",
        "        tensor = torch.from_numpy(np.array(np_array))\n",
        "        return tensor.type(dtype, non_blocking=True).to(self.device, non_blocking=True)\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        if self.top < 1:\n",
        "            experiences.sort(key=lambda e:e[2])\n",
        "            experiences = experiences[int((self.top*self.batch_size)-1):int(self.batch_size-1)]\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "        states, next_states = self._as_tensor(states), self._as_tensor(next_states)\n",
        "        actions, rewards, dones = self._as_tensor(actions, dtype=torch.int64), self._as_tensor(rewards), self._as_tensor(dones, dtype=torch.uint8)\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "        \n",
        "\n",
        "    def save(self, filename):\n",
        "        pickle.dump(self.memory, open(filename, 'wb') )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YLCgDIQ1RfH"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_SOXNZ20yFQ4"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, args):\n",
        "        \"\"\"\n",
        "        args = {\n",
        "            input_shape:  (tuple) dimension of each state (C, H, W)\n",
        "            action_size (int): dimension of each action\n",
        "            seed (int): random seed\n",
        "            device(string): Use Gpu or CPU\n",
        "            buffer_size (int): replay buffer size\n",
        "            batch_size (int):  torch minibatch size\n",
        "            gamma (float): discount factor\n",
        "            lr (float): learning rate \n",
        "            update_every (int): how often to update the network\n",
        "            replay_after (int): After which replay to be started\n",
        "            model(Model): Pytorch Model\n",
        "            base_filename (str): base filename to save models to\n",
        "        }\n",
        "        \"\"\"\n",
        "        self.input_shape = args['input_shape']\n",
        "        self.action_size = args['action_size']\n",
        "        self.device = args['device']\n",
        "        self.buffer_size = args['buffer_size']\n",
        "        self.batch_size = args['batch_size']\n",
        "        self.gamma = args['gamma']\n",
        "        self.lr = args['lr']\n",
        "        self.learn_every = args['learn_every']\n",
        "        self.replay_after = args['replay_after']\n",
        "        self.network = args['model']\n",
        "        self.tau = args['tau']\n",
        "        self.base_filename = args['base_filename']\n",
        "        episodes = args['episodes'] if 'episodes' in args else 10000\n",
        "        \n",
        "        \n",
        "        # Q-Network\n",
        "        self.policy_net = self.network(self.input_shape, self.action_size).to(self.device)\n",
        "        self.target_net = self.network(self.input_shape, self.action_size).to(self.device)\n",
        "        self.optimizer = torch.optim.AdamW(self.policy_net.parameters(), lr=self.lr, )\n",
        "        self.loss = nn.SmoothL1Loss()\n",
        "        self.losses = torch.zeros(int(episodes/self.learn_every+2)).to(self.device)\n",
        "        self.nextLossIdx = torch.tensor([0],dtype=torch.uint8).to(self.device)\n",
        "\n",
        "        self.memoryTop = args['memory_top'] if 'memory_top' in args else False\n",
        "        self.memory = ReplayMemory(self.buffer_size, self.batch_size, self.device, top=self.memoryTop)\n",
        "        \n",
        "        self.time_step = 0\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.policy_net.state_dict(), f\"{self.base_filename}.policy.net\")\n",
        "        torch.save(self.target_net.state_dict(), f\"{self.base_filename}.target.net\")\n",
        "        self.memory.save(f\"{self.base_filename}.memory\")\n",
        "\n",
        "    def load(self):\n",
        "        self.memory = ReplayMemory(self.buffer_size, self.batch_size, self.device, f\"{self.base_filename}.memory\", top=self.memoryTop)\n",
        "        self.policy_net.load_state_dict(torch.load(f\"{self.base_filename}.policy.net\"))\n",
        "        self.target_net.load_state_dict(torch.load(f\"{self.base_filename}.target.net\"))\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        \n",
        "        # Learn every \"learn_every\" time steps.\n",
        "        self.time_step = (self.time_step + 1) % self.learn_every\n",
        "        if self.time_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > self.replay_after:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences)\n",
        "\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        if torch.rand(1).item() < eps: # Epsilon-greedy action selection\n",
        "            return torch.randint(self.action_size, (1,)).item(), 2\n",
        "        \n",
        "        self.policy_net.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            state = torch.from_numpy(state).unsqueeze(0).to(self.device)\n",
        "            action_values = self.policy_net(state)\n",
        "            action_selection = action_values.detach().argmax().cpu().numpy()\n",
        "        self.policy_net.train() # set the model back to training mode\n",
        "        return action_selection, 1\n",
        "            \n",
        "    def learn(self, experiences): #input = 1 mini-batch\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Get expected Q values from the policy, and max predicted Q values from the target\n",
        "        Q_expected = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        targets_next = self.target_net(next_states).detach().max(dim=1)[0]\n",
        "        \n",
        "        # Calculate the Q value\n",
        "        # Multiply by (1 - done) to zero out the next state Q values if the game ended.\n",
        "        Q_targets = rewards + (self.gamma * targets_next * (1 - dones))\n",
        "        \n",
        "        # optimise the model, by minimising the loss\n",
        "        loss = self.loss(Q_expected, Q_targets)\n",
        "        self.losses[self.nextLossIdx.item()] = loss\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.soft_update(self.policy_net, self.target_net, self.tau)\n",
        "\n",
        "    \n",
        "    # θ'=θ×τ+θ'×(1−τ)\n",
        "    def soft_update(self, policy_model, target_model, tau):\n",
        "        for target_param, policy_param in zip(target_model.parameters(), policy_model.parameters()):\n",
        "            target_param.data.copy_(tau*policy_param.data + (1.0-tau)*target_param.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj38QqcR2tw1"
      },
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IuCDHnnL2M8i"
      },
      "outputs": [],
      "source": [
        "def epsilon_decrease_func(start, end, decay):\n",
        "    def epsilon_decrease(i):\n",
        "        return end + (start - end) * math.exp(-1. * i / decay)\n",
        "    return epsilon_decrease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nNC_i9PE1TiX"
      },
      "outputs": [],
      "source": [
        "def trainDQN(agent: DQNAgent, epsilon_decrease, env, n_episodes=1000, network='DQNcnn', start_epoch = 0, save_every=100, plot_every=500, log_eps=False):\n",
        "    print(f\"Training for {n_episodes} episodes, train every {agent.learn_every} episodes = {int(n_episodes/agent.learn_every)} train epoch\")\n",
        "    scores = []\n",
        "    stack_frames = get_stack_frames(network)\n",
        "    for i_episode in range(start_epoch + 1, start_epoch + n_episodes + 1):\n",
        "        state = stack_frames(None, env.reset(), is_new=True)\n",
        "        score, done = 0, False\n",
        "        eps = epsilon_decrease(i_episode)\n",
        "        while not done:\n",
        "            action, _ = agent.act(state, eps)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            next_state = stack_frames(state, next_state, is_new=False)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "        scores.append(score)\n",
        "\n",
        "        print(f'\\rEpisode {i_episode}\\tScore: {scores[-1]}\\tAverage Score: {round(np.mean(scores[-20:]), 2)}\\t eps:{round(eps, 2)}\\t ', end=\"\") #log_eps_str\n",
        "        \n",
        "        if i_episode % plot_every == 0:\n",
        "            print(f'\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores[-plot_every:])}')\n",
        "            _ = plt.hist(scores[-plot_every:], bins=30)\n",
        "            plt.show()\n",
        "            evalDQN(agent, 0.1, n_episodes=50,  network=network)\n",
        "        if i_episode % save_every == 0:\n",
        "            agent.save()\n",
        "    agent.save()\n",
        "    return scores\n",
        "\n",
        "def evalDQN(agent, eps, env, n_episodes=50,  network='DQNcnn'):\n",
        "    scores = []\n",
        "    stack_frames = get_stack_frames(network)\n",
        "    high_score, high_score_action_types = 0, (0.0,0.0)\n",
        "    for i_episode in range(n_episodes):\n",
        "        state = stack_frames(None, env.reset(), is_new=True)\n",
        "        score, done = 0, False\n",
        "        actionTypes, actionTypesIdx = np.zeros(50000, dtype=np.uint8), 0\n",
        "        while not done:\n",
        "            action, actionType = agent.act(state, eps)\n",
        "            actionTypes[actionTypesIdx] = actionType\n",
        "            actionTypesIdx += 1\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            state = stack_frames(state, next_state, is_new=False)\n",
        "        scores.append(score)\n",
        "        if score > high_score:\n",
        "            high_score = score\n",
        "            high_score_action_types = (np.count_nonzero(actionTypes == 2)/actionTypesIdx, np.count_nonzero(actionTypes == 1)/actionTypesIdx) \n",
        "        log_eps_str = f'rand({round(np.count_nonzero(actionTypes[actionTypes == 2])/actionTypesIdx,2)}) nural({round(np.count_nonzero(actionTypes[actionTypes == 1])/actionTypesIdx,2)})    '\n",
        "        print(f'\\rEpisode {i_episode}\\tScore: {scores[-1]}\\tAverage Score: {round(np.mean(scores[-n_episodes:]), 2)}\\t eps:{round(eps, 2)}\\t {log_eps_str}', end=\"\")\n",
        "    print(f' eval:\\navg score: {np.mean(scores)}  high score: {np.max(scores)} low score: {np.min(scores)}')\n",
        "    iqr25, iqr75, iqrAvg = iqr_mean(scores)\n",
        "    print(f\"interquatile range ({iqr25} -> {iqr75}) mean = {iqrAvg}\")\n",
        "    print(f'high score: {high_score}, rand: {high_score_action_types[0]} nnural: {high_score_action_types[1]}')\n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw-odSlqxgP8"
      },
      "source": [
        "### run train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Vxf8Qb6Nndf2"
      },
      "outputs": [],
      "source": [
        "model = 'DQNcnn' # options are 'DQNcnn' or 'ResNetDQN'\n",
        "\n",
        "Number_of_episodes = 20000\n",
        "save_models_every = 250\n",
        "train_every = 20\n",
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx6Xu8kRndf2"
      },
      "source": [
        "#### training runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "8OuNTFsI3C_z",
        "outputId": "a3fdcd9e-8055-4e8c-c221-ae43098ed536"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/sgmsempl/.conda/envs/torch/lib/python3.9/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /users/sgmsempl/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = wrap_env(gym.make(env_name), record_every=1000)\n",
        "\n",
        "agent_args = {\n",
        "            'input_shape': (4, frame_crops[model], frame_crops[model]),\n",
        "            'action_size': env.action_space.n,\n",
        "            'device': device,\n",
        "            'buffer_size': 100000,\n",
        "            'batch_size': 64,\n",
        "            'gamma': 0.99,\n",
        "            'lr': 1e-4,\n",
        "            'tau': 1e-3,\n",
        "            'learn_every': train_every,\n",
        "            'replay_after': 10000,\n",
        "            'memory_top': 0.25,\n",
        "            'episodes': Number_of_episodes\n",
        "        }\n",
        "\n",
        "if model == 'DQNcnn':\n",
        "    agent_args = {**agent_args, 'model': DQNCnn, 'base_filename': 'atari_models/atari_atlantas_models1-Copy1'}\n",
        "elif model == 'ResNetDQN':\n",
        "    agent_args = {**agent_args, 'model': ResNetDQN, 'base_filename': 'atari_models/atari_atlantas_resnet'}\n",
        "else:\n",
        "    raise Exception('model not found')\n",
        "\n",
        "epsilon_decrease = epsilon_decrease_func(start=0.9, end=0.02, decay=Number_of_episodes/2)\n",
        "# epsilon_decrease = lambda x: 0.05\n",
        "plot_epsilon_func(epsilon_decrease, Number_of_episodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3DpdbkZndf3"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "2XfmuL5rndf4",
        "outputId": "8b896933-73de-4b12-dd79-4485999216bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " eval:\n",
            "avg score: 5036.0  high score: 16800.0\n",
            "high score: 16800.0, rand: 0.11026200873362445 nnural: 0.8897379912663755\n"
          ]
        }
      ],
      "source": [
        "\n",
        "agent = DQNAgent(agent_args)\n",
        "agent.load()\n",
        "scores = trainDQN(agent, epsilon_decrease, env, n_episodes=Number_of_episodes, save_every=save_models_every, plot_every=1000, network=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/sgmsempl/.conda/envs/torch/lib/python3.9/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /users/sgmsempl/eval_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 24\tScore: 6800.0\tAverage Score: 5284.0\t eps:0.08\t rand(0.08) nural(0.92)     eval:\n",
            "avg score: 5284.0  high score: 8700.0 low score: 2500.0\n",
            "interquatile range (4500.0 -> 6000.0) mean = 5237.5\n",
            "high score: 8700.0, rand: 0.07228915662650602 nnural: 0.927710843373494\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[6500.0,\n",
              " 4700.0,\n",
              " 3900.0,\n",
              " 2900.0,\n",
              " 4500.0,\n",
              " 6300.0,\n",
              " 7200.0,\n",
              " 2900.0,\n",
              " 5100.0,\n",
              " 5000.0,\n",
              " 6000.0,\n",
              " 5300.0,\n",
              " 5700.0,\n",
              " 8000.0,\n",
              " 3900.0,\n",
              " 4500.0,\n",
              " 6000.0,\n",
              " 8700.0,\n",
              " 5800.0,\n",
              " 2500.0,\n",
              " 4900.0,\n",
              " 3600.0,\n",
              " 5400.0,\n",
              " 6000.0,\n",
              " 6800.0]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = wrap_env(gym.make(env_name), record_every=1000, folder='./eval_videos')\n",
        "agent = DQNAgent(agent_args)\n",
        "agent.load()\n",
        "evalScores = evalDQN(agent, 0.08, env, n_episodes=25,  network=model)\n",
        "evalScores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "nDIRbuQUndf8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "_ = plt.hist(evalScores, bins=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xzRetiqKndf-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " eval:\n",
            "avg score: 5346.0  high score: 12200.0\n",
            "high score: 12200.0, rand: 0.08440170940170941 nnural: 0.9155982905982906\n"
          ]
        }
      ],
      "source": [
        "evalScores = evalDQN(agent, 0.08, n_episodes=100,  network=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE-yFm4M4DoR"
      },
      "outputs": [],
      "source": [
        "def finalPlay(agent):\n",
        "    env = wrap_env(gym.make(env_name))\n",
        "    score = 0\n",
        "    state = stack_frames(None, env.reset(), True)\n",
        "    while True:\n",
        "        # env.render()\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        score += reward\n",
        "        state = stack_frames(state, next_state)\n",
        "        if done:\n",
        "            print(\"You Final score is:\", score)\n",
        "            break \n",
        "    env.close()\n",
        "finalPlay(agent)\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyBB-sqS3F0i"
      },
      "source": [
        "### DQN2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeDcleB_WV2W"
      },
      "outputs": [],
      "source": [
        "class DQN2(nn.Module):\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Number of Linear input connections depends on output of conv2d layers\n",
        "        # and therefore the input image size, so compute it.\n",
        "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        print(\"in size\", x.size())\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        print(\"2 size\", x.size())\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        print(\"3 size\", x.size())\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_bgOQObndgD"
      },
      "outputs": [],
      "source": [
        "def get_output_size(in_size, kernel_size, stride=1, padding=0):\n",
        "    integer = int((in_size - kernel_size + 2 * padding) / stride) == ((in_size - kernel_size + 2 * padding) / stride)\n",
        "    if not integer:\n",
        "        print(\"not int\", ((in_size - kernel_size + 2 * padding) / stride))\n",
        "    return int((in_size - kernel_size + 2 * padding) / stride) + 1\n",
        "\n",
        "get_output_size(in_size=84, kernel_size=3, stride=2, padding=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqqu-qq39Swm"
      },
      "outputs": [],
      "source": [
        "class DQNtst(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super().__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        #in = 299x299\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=0), #out = 149x149\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            # nn.Dropout(0.3),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0), #out = 147x147\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #out = 147x147\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0) #out = 73x73                    \n",
        "        )\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "                  nn.Linear(state_space_dim,64),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Linear(64,64*2),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Linear(64*2,action_space_dim)\n",
        "                )\n",
        "# (33600x3 and 210x64)\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        print('in size=', x.size())\n",
        "        x = self.block1(x)\n",
        "        print('block1 size=', x.size())\n",
        "        return self.linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRwZZtBSS4ti"
      },
      "outputs": [],
      "source": [
        "class DQNCnn(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super().__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "        \n",
        "        self.features = nn.Sequential( # in = 84x84\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        ) # out = 7x7\n",
        "        self.feature_size = 7 * 7 * 64\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, self.num_actions)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bfd5JgWoICcg",
        "zyBEYPP0H-qS",
        "yeaT5VbKKpSE"
      ],
      "include_colab_link": true,
      "name": "working.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "967264ae980203440ddaee0507bc3cd46af5e06b6d5472ede725bb9edfce0d78"
    },
    "kernelspec": {
      "display_name": "Python [conda env:root] *",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
