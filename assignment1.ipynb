{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from numpy import cos, sin\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "# import visualisation\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "import torch.optim as optim\n",
    "torch.backends.cudnn.benchmark = True \n",
    "\n",
    "import torchvision as tv\n",
    "\n",
    "\n",
    "print(\"Using torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:1\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "\n",
    "DATASET_PATH = \"ass1_data/Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [[31, 120, 180], [51, 160, 44]]\n",
    "colors = [(r / 255, g / 255, b / 255) for (r, g, b) in colors]\n",
    "\n",
    "def plot_epochs(train_history, val_history, plotType=\"loss\"):\n",
    "    x = np.arange(1, len(train_history) + 1)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x, val_history, color=colors[1], label=f\"Validation {plotType}\", linewidth=2)\n",
    "    plt.plot(x, train_history, color=colors[0], label=f\"Training {plotType}\", linewidth=2)\n",
    "    plt.ylabel(f'{plotType}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f\"Evolution of the training and validation {plotType}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_output_size(in_size, kernel_size, stride=1, padding=0):\n",
    "    integer = int((in_size - kernel_size + 2 * padding) / stride) == ((in_size - kernel_size + 2 * padding) / stride)\n",
    "    if not integer:\n",
    "        print(\"not int\", ((in_size - kernel_size + 2 * padding) / stride))\n",
    "    return int((in_size - kernel_size + 2 * padding) / stride) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "def gen_transforms(resize=None, imgs_mean=(0.5,), imgs_std=(0.5,)):\n",
    "    tst_trans = [tv.transforms.ToTensor()]\n",
    "    if resize is not None:\n",
    "        tst_trans.append(tv.transforms.Resize(resize[0]))\n",
    "        tst_trans.append(tv.transforms.CenterCrop(resize[1]))\n",
    "    tst_trans.append(tv.transforms.Normalize(mean=imgs_mean, std=imgs_std))\n",
    "    return tv.transforms.Compose(tst_trans)\n",
    "\n",
    "def get_train_test_data(image_resize=None):\n",
    "    transforms = gen_transforms(image_resize)\n",
    "    trainData = tv.datasets.DatasetFolder(\n",
    "        root=f'{DATASET_PATH}/training',\n",
    "        transform=transforms,\n",
    "        extensions=(\"rgb.png\"),\n",
    "        loader=tv.datasets.folder.default_loader)\n",
    "    # a = ({\n",
    "    #     loader: Callable[[str], Any],\n",
    "    #     target_transform: Optional[Callable] = None,\n",
    "    #     is_valid_file: Optional[Callable[[str], bool]] = None,\n",
    "    # })\n",
    "    return trainData\n",
    "\n",
    "\n",
    "def get_train_loader(batchSize, resize=None):\n",
    "    trainData = get_train_test_data(resize)\n",
    "    trainLoader = td.DataLoader(trainData, batch_size=batchSize, shuffle=True, drop_last=False, num_workers=4)\n",
    "    return trainLoader\n",
    "\n",
    "trainLoader = get_train_loader(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLoss():\n",
    "    return nn.MSELoss()\n",
    "\n",
    "def getOptimiser(model, learningRate):\n",
    "    #optimiser = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    optimiser = optim.Adam(model.parameters(), lr=learningRate)\n",
    "    return optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, batch_size, n_epochs, learning_rate, resize=None):\n",
    "    print(f\"batch size: {batch_size}\")\n",
    "    print(f\"n epochs: {n_epochs}\")\n",
    "    print(f\"learning rate: {learning_rate}\")\n",
    "    print(\"=\"*20)\n",
    "    \n",
    "    trainLoader = get_train_loader(batch_size, resize)\n",
    "    lossFn = getLoss()\n",
    "    optimizer = getOptimiser(model, learning_rate)\n",
    "\n",
    "    train_history = []\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    model = model.to(device) # Move model to gpu\n",
    "    model.train() # Set the model to training mode (for DropOut)\n",
    "\n",
    "    for epoch in range(n_epochs):  # loop over the dataset for n_epoch\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        with tqdm(trainLoader, unit=\"batch\") as tepoch:\n",
    "            for inputs, targets in tepoch:   #for each batch\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                # Move tensors to gpu\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "                # _, predicted = outputs.argmax(dim=1, keepdim=True).squeeze()\n",
    "                #  = torch.max(outputs, 1)\n",
    "                print('out', outputs.size(), 'targets', targets.size())\n",
    "                loss = lossFn(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # add to running totals\n",
    "                epoch_loss += loss.item()\n",
    "        epoch_loss = epoch_loss / len(trainLoader)\n",
    "        # print(f\"Epoch #{epoch + 1} train_loss: {round(epoch_loss,2)} accuracy: {round(epoch_accuracy,1)}% took: {round(time.time() - start_time,2)}s\") \n",
    "        train_history.append(epoch_loss)\n",
    "\n",
    "    print(f\"Training Finished, took {round(time.time() - training_start_time,1)}s\")\n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cnn_output_size(259, kernel_size=3, stride=1, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepConvNN, self).__init__()\n",
    "        #in = 1047 x 1047\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=0), #out = 523x523\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=0), #out = 521x521\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0), #out = 519x519\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0) #out = 259x259                 \n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), #out = 259x259\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=0), #out = 129x129\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #out = 129x129\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "            # MaxPool could be used instead of stride 2\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0), #out = 127x127\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0), #out = 63x63\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=0),  #out = 61x61\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=0), #out = 59x59\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=0), #out = 29x29\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=0), #out = 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # self.pool = nn.AdaptiveAvgPool2d((4,4))\n",
    "        self.flattened_size = 256 * 7 * 7 #36864\n",
    "\n",
    "        self.regressionBlock = nn.Sequential(\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.Linear(self.flattened_size, 512),\n",
    "            nn.Linear(512, 5),\n",
    "            # nn.Dropout(0.5), \n",
    "            # nn.Linear(2048, 5)  # We also found two fully connected layers to reduce accuracy\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "        x = self.regressionBlock(x)\n",
    "        print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 4\n",
      "n epochs: 5\n",
      "learning rate: 0.01\n",
      "====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952551322b5d42c1b6dd3e58c773f29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5])\n",
      "out torch.Size([4, 5]) targets torch.Size([4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/sgmsempl/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m resize \u001b[39m=\u001b[39m (\u001b[39m1047\u001b[39m, \u001b[39m1047\u001b[39m)\n\u001b[1;32m      6\u001b[0m nnet \u001b[39m=\u001b[39m DeepConvNN()\n\u001b[0;32m----> 8\u001b[0m trains_loss \u001b[39m=\u001b[39m train(nnet, BATCH_SIZE, EPOCHS, LEARNING_RATE, resize)\n",
      "\u001b[1;32m/Users/mikesemple/Google Drive/uni/341 - Robot Perception/assignment1/assignment1.ipynb Cell 9'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, batch_size, n_epochs, learning_rate, resize)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mikesemple/Google%20Drive/uni/341%20-%20Robot%20Perception/assignment1/assignment1.ipynb#ch0000008?line=29'>30</a>\u001b[0m \u001b[39m# _, predicted = outputs.argmax(dim=1, keepdim=True).squeeze()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mikesemple/Google%20Drive/uni/341%20-%20Robot%20Perception/assignment1/assignment1.ipynb#ch0000008?line=30'>31</a>\u001b[0m \u001b[39m#  = torch.max(outputs, 1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mikesemple/Google%20Drive/uni/341%20-%20Robot%20Perception/assignment1/assignment1.ipynb#ch0000008?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mout\u001b[39m\u001b[39m'\u001b[39m, outputs\u001b[39m.\u001b[39msize(), \u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m, targets\u001b[39m.\u001b[39msize())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mikesemple/Google%20Drive/uni/341%20-%20Robot%20Perception/assignment1/assignment1.ipynb#ch0000008?line=32'>33</a>\u001b[0m loss \u001b[39m=\u001b[39m lossFn(outputs, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mikesemple/Google%20Drive/uni/341%20-%20Robot%20Perception/assignment1/assignment1.ipynb#ch0000008?line=33'>34</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mikesemple/Google%20Drive/uni/341%20-%20Robot%20Perception/assignment1/assignment1.ipynb#ch0000008?line=34'>35</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/loss.py:529\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/loss.py?line=527'>528</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/loss.py?line=528'>529</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/functional.py:3261\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/functional.py?line=3257'>3258</a>\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/functional.py?line=3258'>3259</a>\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/functional.py?line=3260'>3261</a>\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[1;32m   <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/functional.py?line=3261'>3262</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/functional.py:75\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/functional.py?line=72'>73</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/functional.py?line=73'>74</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[0;32m---> <a href='file:///Users/mikesemple/.conda/envs/torch/lib/python3.9/site-packages/torch/functional.py?line=74'>75</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.01\n",
    "resize = (1047, 1047)\n",
    "\n",
    "nnet = DeepConvNN()\n",
    "\n",
    "trains_loss = train(nnet, BATCH_SIZE, EPOCHS, LEARNING_RATE, resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torch] *",
   "language": "python",
   "name": "conda-env-.conda-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
