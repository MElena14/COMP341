{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MElena14/COMP341/blob/main/assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfd5JgWoICcg"
      },
      "source": [
        "#### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k3dxF3QaJqeD",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/sgmsempl/.conda/envs/torch/lib/python3.9/site-packages/ale_py/roms/__init__.py:89: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
            "  ROMS = resolve_roms()\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import math\n",
        "from collections import deque, namedtuple\n",
        "import cv2\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm\n",
        "import PIL\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyBEYPP0H-qS"
      },
      "source": [
        "#### Render OpenAI Gym Environments from CoLab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "AL2uvFBoH4ji",
        "outputId": "4238a76d-4f8d-48ff-c34b-b1af70f9ff7a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "not in colab\n",
            "Using device cuda:1\n"
          ]
        }
      ],
      "source": [
        "# HIDE OUTPUT\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
        "    env_name = \"Atlantis-v0\"\n",
        "    !wget http://www.atarimania.com/roms/Roms.rar \n",
        "    !unrar x -o+ /content/Roms.rar >/dev/nul\n",
        "    !python -m atari_py.import_roms /content/ROMS >/dev/nul\n",
        "\n",
        "    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "    !apt-get update > /dev/null 2>&1\n",
        "    !apt-get install cmake > /dev/null 2>&1\n",
        "    !pip install --upgrade setuptools 2>&1\n",
        "    !pip install ez_setup > /dev/null 2>&1\n",
        "    !pip install gym[atari] > /dev/null 2>&1\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    env_name = \"ALE/Atlantis-v5\"\n",
        "    print('not in colab')\n",
        "    device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:1\")\n",
        "print(\"Using device\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjkT6v3pHnVv",
        "outputId": "5927ff8d-d5de-46f3-f192-70708746f48f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
            "[Powered by Stella]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action Space: Discrete(4)\n",
            "possible actions: ['NOOP', 'FIRE', 'RIGHTFIRE', 'LEFTFIRE']\n",
            "Observation Space: Box(0, 255, (210, 160, 3), uint8)\n",
            "Max Episode Steps: 27000\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: None\n"
          ]
        }
      ],
      "source": [
        "def query_environment(name):\n",
        "  env = gym.make(name)\n",
        "  spec = gym.spec(name)\n",
        "  print(f\"Action Space: {env.action_space}\")\n",
        "  print(f\"possible actions: {env.unwrapped.get_action_meanings()}\")\n",
        "  print(f\"Observation Space: {env.observation_space}\")\n",
        "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "  print(f\"Reward Range: {env.reward_range}\")\n",
        "  print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
        "  \n",
        "query_environment(env_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8X-yeVrRHpTf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# display = Display(visible=0, size=(1400, 900))\n",
        "# display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment \n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  print(mp4list[-1])\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[-1]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "if IN_COLAB:\n",
        "    from gym.wrappers import Monitor\n",
        "    def wrap_env(env, record_every=1, folder='./video'):\n",
        "        env = Monitor(env, folder, force=True)\n",
        "        return env\n",
        "else:\n",
        "    from gym.wrappers.record_video import RecordVideo\n",
        "    def wrap_env(env, record_every=1, folder='./video'):\n",
        "        env = RecordVideo(env, folder, episode_trigger=lambda i: i % record_every == 0)\n",
        "        return env\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvpErA1hITFs"
      },
      "source": [
        "#### inital random agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AVzkJJHendfi",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def show_random_agent():\n",
        "  env = wrap_env(gym.make(env_name))\n",
        "\n",
        "  observation = env.reset()\n",
        "  score = 0\n",
        "  while True:\n",
        "      env.render()\n",
        "      action = env.action_space.sample()\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      score += reward\n",
        "      if done:\n",
        "        print(f\"finished! random agent's score is {score}\")\n",
        "        break\n",
        "  env.close()\n",
        "  show_video()\n",
        "# show_random_agent()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkV2DFe4JwC9"
      },
      "source": [
        "#### Deep QN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Mx-ZVJ4s4nV_",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class DQNCnn(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super().__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "        \n",
        "        self.features = nn.Sequential( # in = 84x84\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        ) # out = 7x7\n",
        "        self.feature_size = 7 * 7 * 64\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, self.num_actions)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg5IwV9Gndfl"
      },
      "source": [
        "#### ResNetDQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krpJGCQ-ndfl"
      },
      "source": [
        "##### ResNetBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SsP2JnGWndfm",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, depth_in, activation_func, depth_out=-1):\n",
        "        super().__init__()\n",
        "        self.downSampleResidul = True\n",
        "        if depth_out == -1:\n",
        "            depth_out = depth_in\n",
        "            self.downSampleResidul = False\n",
        "\n",
        "        self.resBlock = nn.Sequential(\n",
        "            nn.BatchNorm2d(depth_in),\n",
        "            activation_func(inplace=True),\n",
        "            nn.Conv2d(depth_in, depth_out, kernel_size=3, padding=1, stride=1 if not self.downSampleResidul else 2, bias=False),\n",
        "            nn.BatchNorm2d(depth_out),\n",
        "            activation_func(inplace=True),\n",
        "            nn.Conv2d(depth_out, depth_out, kernel_size=3, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.downsampleRes = nn.Sequential(\n",
        "            nn.BatchNorm2d(depth_in),\n",
        "            activation_func(inplace=True),\n",
        "            nn.Conv2d(depth_in, depth_out, kernel_size=1, stride=2, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.resBlock(x)\n",
        "        if self.downSampleResidul:\n",
        "            x = self.downsampleRes(x)\n",
        "        return z + x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTcVbQMrndfo"
      },
      "source": [
        "##### ResNetDQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9zx6rGfDndfp",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class ResNetDQN(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "        super(ResNetDQN, self).__init__()\n",
        "        self.block1 = nn.Sequential(    #in = 159x159\n",
        "            nn.Conv2d(input_shape[0], 16, kernel_size=3, stride=2, padding=0), #out = 79x79\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=0), #out = 77x77\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0), #out = 75x75\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0) #out = 37x37\n",
        "        )\n",
        "        self.featureExtractionBlock = nn.Sequential(\n",
        "            ResNetBlock(32, nn.SiLU, 64),  #out = 19x19\n",
        "            ResNetBlock(64, nn.SiLU),      #out = 19x19\n",
        "            ResNetBlock(64, nn.SiLU),      #out = 19x19 \n",
        "            ResNetBlock(64, nn.SiLU, 128), #out = 10x10\n",
        "            ResNetBlock(128, nn.SiLU),     #out = 10x10\n",
        "        )\n",
        "        self.flattened_size = 128 * 10 * 10\n",
        "        self.regressionBlock = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(self.flattened_size, 1024),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Linear(1024, self.num_actions),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.featureExtractionBlock(x)\n",
        "        x = x.view(-1, self.flattened_size)\n",
        "        x = self.regressionBlock(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSFkzLbg0p7q"
      },
      "source": [
        "#### utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "uS4Ms1_30tbT",
        "outputId": "3dd6f4ca-7ad1-4481-d2b0-0a4f33a5262c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJ8AAACfCAAAAADCcFQ8AAABi0lEQVR4nO3YTStEUQCH8b9pmnE1490UEqGMlJSXvGzEUk1ZsrC08AHsLSylfAFlb2dhaeslZYEkJDWMKbOYRnfMMFY0I+rEnEE9v9VdnNN5Op3OvV0JAAAAAAAAAAAAAAAAgC1lpgOdSin29iAlnmwVFfKaDuyJ6HlJksLTkrR+ZauokOn+TYZ25ZldTUrBwTlp7cS1mvXOYzYsFEzcPl1mWvyqqX/JZM4uSpRnun9T9/utE+ua34qOjkhasdqUz/D8teRGZiq6F8M9WadcD8t2m/IZ9jm+QJMCCtRKkpuwWVTIsM/n9TrKyu9Ikt9m0AeG5y8SO+gY33AXtqPDQ8ql1+xG5THcv9Oqxtheuu8w7l64wU71nZToeja9X84zzY/HvvBRUjd7p6lUb4Xxe+eHjNdp79fLpiS1DUrSTtxWUSHjvmCdcteSFKiXpLtSXdAAAPxVHsOvkM+mFjHjSwNdpVjl+xqqf7sAAAAAAAAAAPCPVA7ZX+Mn/++rxoqWAQAAAAAAAAAAUDSvrk5NW+tNUk4AAAAASUVORK5CYII=",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=159x159 at 0x7F5194221FD0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# taken from https://github.com/deepanshut041/Reinforcement-Learning/blob/master/algos/preprocessing/stack_frame.py\n",
        "def preprocess_frame(screen, exclude, output):\n",
        "    \"\"\"Preprocess Image.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            screen (array): RGB Image\n",
        "            exclude (tuple): Section to be croped (UP, RIGHT, DOWN, LEFT)\n",
        "            output (int): Size of output image\n",
        "        \"\"\"\n",
        "    # TConver image to gray scale\n",
        "    screen = cv2.cvtColor(screen, cv2.COLOR_RGB2GRAY)\n",
        "    \n",
        "    #Crop screen[Up: Down, Left: right] \n",
        "    screen = screen[exclude[0]:exclude[2], exclude[3]:exclude[1]]\n",
        "    \n",
        "    # Convert to float, and normalized\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    \n",
        "    # Resize image to 84 * 84\n",
        "    screen = cv2.resize(screen, (output, output), interpolation = cv2.INTER_AREA)\n",
        "    return screen\n",
        "\n",
        "def stack_frame(stacked_frames, frame, is_new):\n",
        "    \"\"\"Stacking Frames.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            stacked_frames (array): Four Channel Stacked Frame\n",
        "            frame: Preprocessed Frame to be added\n",
        "            is_new: Is the state First\n",
        "        \"\"\"\n",
        "    if is_new:\n",
        "        stacked_frames = np.stack(arrays=[frame, frame, frame, frame])\n",
        "        stacked_frames = stacked_frames\n",
        "    else:\n",
        "        stacked_frames[0] = stacked_frames[1]\n",
        "        stacked_frames[1] = stacked_frames[2]\n",
        "        stacked_frames[2] = stacked_frames[3]\n",
        "        stacked_frames[3] = frame\n",
        "    \n",
        "    return stacked_frames\n",
        "\n",
        "frame_crops = {'DQNcnn': 84, 'ResNetDQN': 159}\n",
        "\n",
        "def get_stack_frames(net='DQNcnn'):\n",
        "    size = frame_crops[net]\n",
        "    def stack_frames(frames, state, is_new=False):\n",
        "        frame = preprocess_frame(state, (0, -10, -100, 9), size)\n",
        "        frames = stack_frame(frames, frame, is_new)\n",
        "        return frames\n",
        "    return stack_frames\n",
        "\n",
        "def show_cropped_image():\n",
        "    env = gym.make(env_name)\n",
        "    observation = env.reset()\n",
        "    t, done = 0, False\n",
        "    while not done and t < 260:\n",
        "      # env.render()\n",
        "      action = env.action_space.sample() \n",
        "      observation, reward, done, info = env.step(action)\n",
        "      \n",
        "      t += 1\n",
        "                                # (UP, RIGHT, DOWN, LEFT)\n",
        "    f = preprocess_frame(observation, (0, -10, -100, 9), 159)\n",
        "    return PIL.Image.fromarray(np.uint8(f * 255), mode=\"L\")\n",
        "show_cropped_image()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gQK4tV3qndfu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def plot_epsilon_func(eps_func, n_episodes=1000):\n",
        "    episodes = range(n_episodes)\n",
        "    eps = [eps_func(i) for i in episodes]\n",
        "    plt.plot(episodes, eps)\n",
        "    plt.show()\n",
        "\n",
        "def iqr_mean(scores):\n",
        "    scores = np.array(scores)\n",
        "    q25, q75 = np.percentile(scores, [25, 75])\n",
        "    meaners = scores[np.logical_and(scores > q25, scores < q75)]\n",
        "    return q25, q75, meaners.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtzA_Zt7tTOs"
      },
      "source": [
        "### memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "W14ncSaOKCVj",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "from numpy.typing import NDArray\n",
        "class ReplayMemory:\n",
        "    def __init__(self, buffer_size, batch_size, device, filename=None, top=1):\n",
        "        self.memory = deque(maxlen=buffer_size) if filename is None else pickle.load(open(filename, 'rb'))\n",
        "        self.batch_size = int(batch_size/top) if top < 1 else batch_size\n",
        "        self.top = top\n",
        "        self.device = device\n",
        "        self.mean = lambda l: sum(l)/len(l)\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        Experience = (state, action, reward, next_state, float(done))\n",
        "        self.memory.append(Experience)\n",
        "    def _as_tensor(self, np_array:NDArray, dtype=torch.float):\n",
        "        tensor = torch.from_numpy(np.array(np_array))\n",
        "        return tensor.type(dtype, non_blocking=True).to(self.device, non_blocking=True)\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        if self.top < 1:\n",
        "            experiences.sort(key=lambda e:e[2], reverse=True)\n",
        "            experiences = experiences[:int((self.top*self.batch_size))]\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "        states, next_states = self._as_tensor(states), self._as_tensor(next_states)\n",
        "        actions, rewards, dones = self._as_tensor(actions, dtype=torch.int64), self._as_tensor(rewards), self._as_tensor(dones, dtype=torch.uint8)\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "        \n",
        "\n",
        "    def save(self, filename):\n",
        "        pickle.dump(self.memory, open(filename, 'wb') )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YLCgDIQ1RfH"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_SOXNZ20yFQ4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# our implimentation is based on https://github.com/deepanshut041/Reinforcement-Learning/blob/master/algos/agents/dqn_agent.py\n",
        "# we have improved this implimentation and added load and save functions\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, args):\n",
        "        self.input_shape = args['input_shape']\n",
        "        self.action_size = args['action_size']\n",
        "        self.device = args['device']\n",
        "        self.buffer_size = args['buffer_size']\n",
        "        self.batch_size = args['batch_size']\n",
        "        self.gamma = args['gamma']\n",
        "        self.lr = args['lr']\n",
        "        self.learn_every = args['learn_every']\n",
        "        self.replay_after = args['replay_after']\n",
        "        self.network = args['model']\n",
        "        self.tau = args['tau']\n",
        "        self.base_filename = args['base_filename']\n",
        "        episodes = args['episodes'] if 'episodes' in args else 10000\n",
        "        \n",
        "        \n",
        "        # Q-Network\n",
        "        self.policy_net = self.network(self.input_shape, self.action_size).to(self.device)\n",
        "        self.target_net = self.network(self.input_shape, self.action_size).to(self.device)\n",
        "        self.optimizer = torch.optim.AdamW(self.policy_net.parameters(), lr=self.lr)\n",
        "        self.loss = nn.SmoothL1Loss()\n",
        "        self.losses = torch.zeros(int(episodes/self.learn_every+2)).to(self.device)\n",
        "        self.nextLossIdx = torch.tensor([0],dtype=torch.uint8).to(self.device)\n",
        "\n",
        "        self.memoryTop = args['memory_top'] if 'memory_top' in args else False\n",
        "        self.memory = ReplayMemory(self.buffer_size, self.batch_size, self.device, top=self.memoryTop)\n",
        "        \n",
        "        self.time_step = 0\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.policy_net.state_dict(), f\"{self.base_filename}.policy.net\")\n",
        "        torch.save(self.target_net.state_dict(), f\"{self.base_filename}.target.net\")\n",
        "        self.memory.save(f\"{self.base_filename}.memory\")\n",
        "\n",
        "    def load(self):\n",
        "        self.memory = ReplayMemory(self.buffer_size, self.batch_size, self.device, f\"{self.base_filename}.memory\", top=self.memoryTop)\n",
        "        self.policy_net.load_state_dict(torch.load(f\"{self.base_filename}.policy.net\"))\n",
        "        self.target_net.load_state_dict(torch.load(f\"{self.base_filename}.target.net\"))\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        \n",
        "        # Learn every \"learn_every\" time steps.\n",
        "        self.time_step = (self.time_step + 1) % self.learn_every\n",
        "        if self.time_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > self.replay_after:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences)\n",
        "\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        if torch.rand(1).item() < eps: # Epsilon-greedy action selection\n",
        "            return torch.randint(self.action_size, (1,)).item(), 2\n",
        "        \n",
        "        self.policy_net.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            state = torch.from_numpy(state).unsqueeze(0).to(self.device)\n",
        "            action_values = self.policy_net(state)\n",
        "            action_selection = action_values.detach().argmax().cpu().numpy()\n",
        "        self.policy_net.train() # set the model back to training mode\n",
        "        return action_selection, 1\n",
        "            \n",
        "    def learn(self, experiences): #input = 1 mini-batch\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Get expected Q values from the policy, and max predicted Q values from the target\n",
        "        Q_expected = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        targets_next = self.target_net(next_states).detach().max(dim=1)[0]\n",
        "        \n",
        "        # Calculate the Q value\n",
        "        # Multiply by (1 - done) to zero out the next state Q values if the game ended.\n",
        "        Q_targets = rewards + (self.gamma * targets_next * (1 - dones))\n",
        "        \n",
        "        # optimise the model, by minimising the loss\n",
        "        loss = self.loss(Q_expected, Q_targets)\n",
        "        self.losses[self.nextLossIdx.item()] = loss\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.soft_update(self.policy_net, self.target_net, self.tau)\n",
        "    \n",
        "    def soft_update(self, policy_model, target_model, tau):\n",
        "        for target_param, policy_param in zip(target_model.parameters(), policy_model.parameters()):\n",
        "            target_param.data.copy_(tau*policy_param.data + (1.0-tau)*target_param.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj38QqcR2tw1"
      },
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IuCDHnnL2M8i",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def epsilon_decrease_func(start, end, decay):\n",
        "    def epsilon_decrease(i):\n",
        "        return end + (start - end) * math.exp(-1. * i / decay)\n",
        "    return epsilon_decrease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nNC_i9PE1TiX",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def trainDQN(agent: DQNAgent, epsilon_decrease, env, n_episodes=1000, network='DQNcnn', start_epoch = 0, save_every=100, plot_every=500, log_eps=False):\n",
        "    print(f\"Training for {n_episodes} episodes, train every {agent.learn_every} episodes = {int(n_episodes/agent.learn_every)} train epoch\")\n",
        "    scores = []\n",
        "    stack_frames = get_stack_frames(network)\n",
        "    for i_episode in range(start_epoch + 1, start_epoch + n_episodes + 1):\n",
        "        state = stack_frames(None, env.reset(), is_new=True)\n",
        "        score, done = 0, False\n",
        "        eps = epsilon_decrease(i_episode)\n",
        "        while not done:\n",
        "            action, _ = agent.act(state, eps)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            next_state = stack_frames(state, next_state, is_new=False)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "        scores.append(score)\n",
        "\n",
        "        print(f'\\rEpisode {i_episode}\\tScore: {scores[-1]}\\tAverage Score: {round(np.mean(scores[-20:]), 2)}\\t eps:{round(eps, 2)}\\t ', end=\"\") #log_eps_str\n",
        "        \n",
        "        if i_episode % plot_every == 0:\n",
        "            print(f'\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores[-plot_every:])}')\n",
        "            _ = plt.hist(scores[-plot_every:], bins=30)\n",
        "            plt.show()\n",
        "            evalDQN(agent, 0.1, n_episodes=50,  network=network)\n",
        "        if i_episode % save_every == 0:\n",
        "            agent.save()\n",
        "    agent.save()\n",
        "    return scores\n",
        "\n",
        "def evalDQN(agent, eps, env, n_episodes=50,  network='DQNcnn'):\n",
        "    scores = []\n",
        "    stack_frames = get_stack_frames(network)\n",
        "    high_score, high_score_action_types = 0, (0.0,0.0)\n",
        "    for i_episode in range(n_episodes):\n",
        "        state = stack_frames(None, env.reset(), is_new=True)\n",
        "        score, done = 0, False\n",
        "        actionTypes, actionTypesIdx = np.zeros(50000, dtype=np.uint8), 0\n",
        "        while not done:\n",
        "            action, actionType = agent.act(state, eps)\n",
        "            actionTypes[actionTypesIdx] = actionType\n",
        "            actionTypesIdx += 1\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            state = stack_frames(state, next_state, is_new=False)\n",
        "        scores.append(score)\n",
        "        if score > high_score:\n",
        "            high_score = score\n",
        "            high_score_action_types = (np.count_nonzero(actionTypes == 2)/actionTypesIdx, np.count_nonzero(actionTypes == 1)/actionTypesIdx) \n",
        "        log_eps_str = f'rand({round(np.count_nonzero(actionTypes[actionTypes == 2])/actionTypesIdx,2)}) neural({round(np.count_nonzero(actionTypes[actionTypes == 1])/actionTypesIdx,2)})    '\n",
        "        print(f'\\rEpisode {i_episode+1}\\tScore: {round(scores[-1])}\\tAverage Score: {round(np.mean(scores[-n_episodes:]), 1)}\\t eps:{round(eps, 2)}\\t {log_eps_str}', end=\"\")\n",
        "    print(f'\\n eval:\\navg score: {np.mean(scores)}  high score: {np.max(scores)} low score: {np.min(scores)}')\n",
        "    iqr25, iqr75, iqrAvg = iqr_mean(scores)\n",
        "    print(f\"interquatile range ({iqr25} -> {iqr75}) mean = {round(iqrAvg, 2)}\")\n",
        "    print(f'high score: {high_score}, rand: {round(high_score_action_types[0]*100, 1)}% neural net: {round(high_score_action_types[1]*100,1)}%')\n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw-odSlqxgP8"
      },
      "source": [
        "### run train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Vxf8Qb6Nndf2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model = 'ResNetDQN' # options are 'DQNcnn' or 'ResNetDQN'\n",
        "\n",
        "Number_of_episodes = 20000\n",
        "save_models_every = 250\n",
        "train_every = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx6Xu8kRndf2"
      },
      "source": [
        "#### training runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "8OuNTFsI3C_z",
        "outputId": "a3fdcd9e-8055-4e8c-c221-ae43098ed536",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/sgmsempl/.conda/envs/torch/lib/python3.9/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /users/sgmsempl/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = wrap_env(gym.make(env_name), record_every=1000)\n",
        "\n",
        "agent_args = {\n",
        "            'input_shape': (4, frame_crops[model], frame_crops[model]),\n",
        "            'action_size': env.action_space.n,\n",
        "            'device': device,\n",
        "            'buffer_size': 1000000,\n",
        "            'batch_size': 64,\n",
        "            'gamma': 0.99,\n",
        "            'lr': 1e-4,\n",
        "            'tau': 1e-3,\n",
        "            'learn_every': train_every,\n",
        "            'replay_after': 10000,\n",
        "            'memory_top': 0.1,\n",
        "            'episodes': Number_of_episodes\n",
        "        }\n",
        "\n",
        "if model == 'DQNcnn':\n",
        "    agent_args = {**agent_args, 'model': DQNCnn, 'base_filename': 'atari_atlantas_models1'}\n",
        "elif model == 'ResNetDQN':\n",
        "    agent_args = {**agent_args, 'model': ResNetDQN, 'base_filename': 'atari_models/atari_atlantas_resnet-best1'} #atari_models/\n",
        "else:\n",
        "    raise Exception('model not found')\n",
        "\n",
        "epsilon_decrease = epsilon_decrease_func(start=0.9, end=0.02, decay=Number_of_episodes/2)\n",
        "plot_epsilon_func(epsilon_decrease, Number_of_episodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3DpdbkZndf3"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "2XfmuL5rndf4",
        "outputId": "8b896933-73de-4b12-dd79-4485999216bd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "agent = DQNAgent(agent_args)\n",
        "agent.load()\n",
        "scores = trainDQN(agent, epsilon_decrease, env, n_episodes=Number_of_episodes, save_every=save_models_every, plot_every=1000, network=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/sgmsempl/.conda/envs/torch/lib/python3.9/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /users/sgmsempl/best_eval_vids folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 200\tScore: 37100\tAverage Score: 23550.5\t eps:0.05\t rand(0.05) neural(0.95)    \n",
            " eval:\n",
            "avg score: 23550.5  high score: 44100.0 low score: 9400.0\n",
            "interquatile range (18575.0 -> 27475.0) mean = 22551.0\n",
            "high score: 44100.0, rand: 4.9% neural net: 95.1%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evalScores = evalDQN(agent, 0.05, env, n_episodes=100,  network=model)\n",
        "np.argmax(evalScores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "nDIRbuQUndf8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(17800.0, 28850.0, 23840.425531914894, 25240.0, 62500.0)"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def get_score_avgs(scores):\n",
        "    _ = plt.hist(scores, bins=10)\n",
        "    return np.min(scores), *iqr_mean(scores), np.mean(scores), np.max(scores)\n",
        "min, q25, q75, iq_mean, mean, max = get_score_avgs(evalScores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "lE-yFm4M4DoR",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def finalPlay(agent):\n",
        "    env = wrap_env(gym.make(env_name))\n",
        "    score = 0\n",
        "    state = stack_frame(None, env.reset(), True)\n",
        "    while True:\n",
        "        env.render()\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        score += reward\n",
        "        state = stack_frame(state, next_state)\n",
        "        if done:\n",
        "            print(\"You Final score is:\", score)\n",
        "            break \n",
        "    env.close()\n",
        "finalPlay(agent)\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bfd5JgWoICcg",
        "zyBEYPP0H-qS",
        "yeaT5VbKKpSE"
      ],
      "include_colab_link": true,
      "name": "working.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "967264ae980203440ddaee0507bc3cd46af5e06b6d5472ede725bb9edfce0d78"
    },
    "kernelspec": {
      "display_name": "Python (env)",
      "language": "python",
      "name": "myenv"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
