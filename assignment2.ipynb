{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MElena14/COMP341/blob/main/assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfd5JgWoICcg"
      },
      "source": [
        "#### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k3dxF3QaJqeD"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "from collections import deque,namedtuple\n",
        "import cv2\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm\n",
        "import PIL\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import gym\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyBEYPP0H-qS"
      },
      "source": [
        "#### Render OpenAI Gym Environments from CoLab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "AL2uvFBoH4ji",
        "outputId": "4238a76d-4f8d-48ff-c34b-b1af70f9ff7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-20 15:36:48--  http://www.atarimania.com/roms/Roms.rar\n",
            "Resolving www.atarimania.com (www.atarimania.com)... 195.154.81.199\n",
            "Connecting to www.atarimania.com (www.atarimania.com)|195.154.81.199|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19583716 (19M) [application/x-rar-compressed]\n",
            "Saving to: ‘Roms.rar’\n",
            "\n",
            "Roms.rar            100%[===================>]  18.68M   638KB/s    in 31s     \n",
            "\n",
            "2022-04-20 15:37:19 (619 KB/s) - ‘Roms.rar’ saved [19583716/19583716]\n",
            "\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-62.1.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed setuptools-62.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda:0\n"
          ]
        }
      ],
      "source": [
        "# HIDE OUTPUT\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
        "    env_name = \"Atlantis-v0\"\n",
        "    !wget http://www.atarimania.com/roms/Roms.rar \n",
        "    !unrar x -o+ /content/Roms.rar >/dev/nul\n",
        "    !python -m atari_py.import_roms /content/ROMS >/dev/nul\n",
        "\n",
        "    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "    !apt-get update > /dev/null 2>&1\n",
        "    !apt-get install cmake > /dev/null 2>&1\n",
        "    !pip install --upgrade setuptools 2>&1\n",
        "    !pip install ez_setup > /dev/null 2>&1\n",
        "    !pip install gym[atari] > /dev/null 2>&1\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    env_name = \"ALE/Atlantis-v5\"\n",
        "    print('not in colab')\n",
        "    device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:1\")\n",
        "print(\"Using device\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjkT6v3pHnVv",
        "outputId": "5927ff8d-d5de-46f3-f192-70708746f48f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(4)\n",
            "possible actions: ['NOOP', 'FIRE', 'RIGHTFIRE', 'LEFTFIRE']\n",
            "Observation Space: Box(0, 255, (210, 160, 3), uint8)\n",
            "Max Episode Steps: 10000\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: None\n"
          ]
        }
      ],
      "source": [
        "def query_environment(name):\n",
        "  env = gym.make(name)\n",
        "  spec = gym.spec(name)\n",
        "  print(f\"Action Space: {env.action_space}\")\n",
        "  print(f\"possible actions: {env.unwrapped.get_action_meanings()}\")\n",
        "  print(f\"Observation Space: {env.observation_space}\")\n",
        "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "  print(f\"Reward Range: {env.reward_range}\")\n",
        "  print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
        "  \n",
        "query_environment(env_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8X-yeVrRHpTf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# display = Display(visible=0, size=(1400, 900))\n",
        "# display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment \n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  print(mp4list[-1])\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[-1]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "if IN_COLAB:\n",
        "    from gym.wrappers import Monitor\n",
        "    def wrap_env(env, record_every=1):\n",
        "        env = Monitor(env, './video', force=True)\n",
        "        return env\n",
        "else:\n",
        "    from gym.wrappers.record_video import RecordVideo\n",
        "    def wrap_env(env, record_every=1):\n",
        "        env = RecordVideo(env, './video', episode_trigger=lambda i: i % record_every == 0)\n",
        "        return env\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvpErA1hITFs"
      },
      "source": [
        "#### inital random agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AVzkJJHendfi"
      },
      "outputs": [],
      "source": [
        "def show_random_agent():\n",
        "  env = wrap_env(gym.make(env_name))\n",
        "\n",
        "  observation = env.reset()\n",
        "  score = 0\n",
        "  while True:\n",
        "      env.render()\n",
        "      action = env.action_space.sample()\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      score += reward\n",
        "      if done:\n",
        "        print(f\"finished! random agent's score is {score}\")\n",
        "        break\n",
        "  env.close()\n",
        "  show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkV2DFe4JwC9"
      },
      "source": [
        "#### Deep QN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Mx-ZVJ4s4nV_"
      },
      "outputs": [],
      "source": [
        "class DQNCnn(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super().__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "        \n",
        "        self.features = nn.Sequential( # in = 84x84\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        ) # out = 7x7\n",
        "        self.feature_size = 7 * 7 * 64\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, self.num_actions)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg5IwV9Gndfl"
      },
      "source": [
        "#### ResNetDQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krpJGCQ-ndfl"
      },
      "source": [
        "##### ResNetBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SsP2JnGWndfm"
      },
      "outputs": [],
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, depth_in, activation_func, depth_out=-1):\n",
        "        super().__init__()\n",
        "        self.downSampleResidul = True\n",
        "        if depth_out == -1:\n",
        "            depth_out = depth_in\n",
        "            self.downSampleResidul = False\n",
        "\n",
        "        self.resBlock = nn.Sequential(\n",
        "            nn.BatchNorm2d(depth_in),\n",
        "            activation_func(inplace=True),\n",
        "            nn.Conv2d(depth_in, depth_out, kernel_size=3, padding=1, stride=1 if not self.downSampleResidul else 2, bias=False),\n",
        "            nn.BatchNorm2d(depth_out),\n",
        "            activation_func(inplace=True),\n",
        "            nn.Conv2d(depth_out, depth_out, kernel_size=3, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.downsampleRes = nn.Sequential(\n",
        "            nn.BatchNorm2d(depth_in),\n",
        "            activation_func(inplace=True),\n",
        "            nn.Conv2d(depth_in, depth_out, kernel_size=1, stride=2, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.resBlock(x)\n",
        "        if self.downSampleResidul:\n",
        "            x = self.downsampleRes(x)\n",
        "        return z + x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HPg5kxuundfn",
        "outputId": "2edfdf27-7973-4d6c-8645-fc33d9cf2687",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not int 9.5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "def get_cnn_output_size(in_size, kernel_size, stride=1, padding=0):\n",
        "    integer = int((in_size - kernel_size + 2 * padding) / stride) == ((in_size - kernel_size + 2 * padding) / stride)\n",
        "    if not integer:\n",
        "        print(\"not int\", ((in_size - kernel_size + 2 * padding) / stride))\n",
        "    return int((in_size - kernel_size + 2 * padding) / stride) + 1\n",
        "get_cnn_output_size(in_size=20, kernel_size=3, stride=2, padding=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTcVbQMrndfo"
      },
      "source": [
        "##### ResNetDQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9zx6rGfDndfp"
      },
      "outputs": [],
      "source": [
        "class ResNetDQN(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "        super(ResNetDQN, self).__init__()\n",
        "        #in = 159x159\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 16, kernel_size=3, stride=2, padding=0), #out = 79x79\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=0), #out = 77x77\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0), #out = 75x75\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0) #out = 37x37\n",
        "        )\n",
        "        \n",
        "        self.featureExtractionBlock = nn.Sequential(\n",
        "            ResNetBlock(32, nn.SiLU, 64),  #out = 19x19\n",
        "            ResNetBlock(64, nn.SiLU),      #out = 19x19\n",
        "            ResNetBlock(64, nn.SiLU),      #out = 19x19 \n",
        "            ResNetBlock(64, nn.SiLU, 128), #out = 10x10\n",
        "            ResNetBlock(128, nn.SiLU),     #out = 10x10\n",
        "        )\n",
        "        # self.pool = nn.AdaptiveAvgPool2d((4,4))\n",
        "        self.flattened_size = 128 * 10 * 10\n",
        "        self.regressionBlock = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(self.flattened_size, 1024),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Linear(1024, self.num_actions),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.featureExtractionBlock(x)\n",
        "        # x = self.pool(x)\n",
        "        x = x.view(-1, self.flattened_size)\n",
        "        x = self.regressionBlock(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSFkzLbg0p7q"
      },
      "source": [
        "#### utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uS4Ms1_30tbT",
        "outputId": "3dd6f4ca-7ad1-4481-d2b0-0a4f33a5262c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=159x159 at 0x7F9FFB5A21D0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJ8AAACfCAAAAADCcFQ8AAAB2ElEQVR4nO3ZsU9TURjG4bdCLlVAaAgpMYJREztW0pSNCTHRwTi7GVcX/gDiyOKsK4mrMcbEgUEZWRiApAssUBJsY6QWtYV7W8tQEW01xH7noDG/Z7q9ad6+Od9tk54jAQAAAAAAwKeYPaInXv9sT/Hn7vM5f+Hd9ojSxqfbWqjbg37F3m+ov9KV0UZNH/Yc9Gllf/4e3Ph2Mb9gzmpn7zcymJ6cn519mCzuNm88y5kzj9nnW4+qe2EpKgfByJeCJNXMkT+wr990+vvl+itzWit7v7PBxQlJWnqnqGJOa2Wfb9/53gNJ24WyvU07e7/Rq4okrfr4dXEx31gz4qs5CAAAAMB/zsH+WrvsrfdPHUU52L9q1z8av6wtJ/+YvKzf9SlJT6ouonz0u3PznKSPDb1+a87yMd/iynBqaXo5M5ANmjfWdjrO8jJfZe/PvLj3KHX08vFix0k++k2Od3WHPQdBTMq/lKSw86+Kj/nmo4ErChVullTeN2b56Le9cyEpqZbL27N8zDd9TZL0ZtdBlpf5NncC/+lDG3gxljr5PT8546XGbzUap/t5AAAAAAAAAAAA+HsSCQ+hDs9XLzVK7sKOnPL52x9zuH5Fzv4AAAAAAAAAAABOdggSvl/KqV5xBQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# taken from https://github.com/deepanshut041/Reinforcement-Learning/blob/master/algos/preprocessing/stack_frame.py\n",
        "def preprocess_frame(screen, exclude, output):\n",
        "    \"\"\"Preprocess Image.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            screen (array): RGB Image\n",
        "            exclude (tuple): Section to be croped (UP, RIGHT, DOWN, LEFT)\n",
        "            output (int): Size of output image\n",
        "        \"\"\"\n",
        "    # TConver image to gray scale\n",
        "    screen = cv2.cvtColor(screen, cv2.COLOR_RGB2GRAY)\n",
        "    \n",
        "    #Crop screen[Up: Down, Left: right] \n",
        "    screen = screen[exclude[0]:exclude[2], exclude[3]:exclude[1]]\n",
        "    \n",
        "    # Convert to float, and normalized\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    \n",
        "    # Resize image to 84 * 84\n",
        "    screen = cv2.resize(screen, (output, output), interpolation = cv2.INTER_AREA)\n",
        "    return screen\n",
        "\n",
        "def stack_frame(stacked_frames, frame, is_new):\n",
        "    \"\"\"Stacking Frames.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            stacked_frames (array): Four Channel Stacked Frame\n",
        "            frame: Preprocessed Frame to be added\n",
        "            is_new: Is the state First\n",
        "        \"\"\"\n",
        "    if is_new:\n",
        "        stacked_frames = np.stack(arrays=[frame, frame, frame, frame])\n",
        "        stacked_frames = stacked_frames\n",
        "    else:\n",
        "        stacked_frames[0] = stacked_frames[1]\n",
        "        stacked_frames[1] = stacked_frames[2]\n",
        "        stacked_frames[2] = stacked_frames[3]\n",
        "        stacked_frames[3] = frame\n",
        "    \n",
        "    return stacked_frames\n",
        "\n",
        "frame_crops = {'DQNcnn': 84, 'ResNetDQN': 159}\n",
        "\n",
        "def get_stack_frames(net='DQNcnn'):\n",
        "    size = frame_crops[net]\n",
        "    def stack_frames(frames, state, is_new=False):\n",
        "        frame = preprocess_frame(state, (0, -10, -100, 9), size)\n",
        "        frames = stack_frame(frames, frame, is_new)\n",
        "        return frames\n",
        "    return stack_frames\n",
        "\n",
        "def show_cropped_image():\n",
        "    env = gym.make(env_name)\n",
        "    observation = env.reset()\n",
        "    t, done = 0, False\n",
        "    while not done and t < 260:\n",
        "      # env.render()\n",
        "      action = env.action_space.sample() \n",
        "      observation, reward, done, info = env.step(action)\n",
        "      \n",
        "      t += 1\n",
        "                                # (UP, RIGHT, DOWN, LEFT)\n",
        "    f = preprocess_frame(observation, (0, -10, -100, 9), 159)\n",
        "    return PIL.Image.fromarray(np.uint8(f * 255), mode=\"L\")\n",
        "show_cropped_image()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gQK4tV3qndfu"
      },
      "outputs": [],
      "source": [
        "def plot_epsilon_func(eps_func, n_episodes=1000):\n",
        "    episodes = range(n_episodes)\n",
        "    eps = [eps_func(i) for i in episodes]\n",
        "    plt.plot(episodes, eps)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtzA_Zt7tTOs"
      },
      "source": [
        "### memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "W14ncSaOKCVj"
      },
      "outputs": [],
      "source": [
        "# Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "from numpy.typing import NDArray\n",
        "class ReplayMemory:\n",
        "    def __init__(self, buffer_size, batch_size, device, filename=None, topHalf=False):\n",
        "        self.memory = deque(maxlen=buffer_size) if filename is None else pickle.load(open(filename, 'rb'))\n",
        "        self.batch_size = batch_size*2 if topHalf else batch_size\n",
        "        self.topHalf = topHalf\n",
        "        self.device = device\n",
        "        self.mean = lambda l: sum(l)/len(l)\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        Experience = (state, action, reward, next_state, float(done))\n",
        "        self.memory.append(Experience)\n",
        "    def _as_tensor(self, np_array:NDArray, dtype=torch.float):\n",
        "        tensor = torch.from_numpy(np.array(np_array))\n",
        "        return tensor.type(dtype, non_blocking=True).to(self.device, non_blocking=True)\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        if self.topHalf:\n",
        "            experiences.sort(key=lambda e:e[2])\n",
        "            experiences = experiences[int(self.batch_size/2):self.batch_size-1]\n",
        "            print(f\"len={len(experiences)}\")\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "        states, next_states = self._as_tensor(states), self._as_tensor(next_states)\n",
        "        actions, rewards, dones = self._as_tensor(actions, dtype=torch.int64), self._as_tensor(rewards), self._as_tensor(dones, dtype=torch.uint8)\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "        \n",
        "\n",
        "    def save(self, filename):\n",
        "        pickle.dump(self.memory, open(filename, 'wb') )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YLCgDIQ1RfH"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_SOXNZ20yFQ4"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, args):\n",
        "        \"\"\"\n",
        "        args = {\n",
        "            input_shape:  (tuple) dimension of each state (C, H, W)\n",
        "            action_size (int): dimension of each action\n",
        "            seed (int): random seed\n",
        "            device(string): Use Gpu or CPU\n",
        "            buffer_size (int): replay buffer size\n",
        "            batch_size (int):  torch minibatch size\n",
        "            gamma (float): discount factor\n",
        "            lr (float): learning rate \n",
        "            update_every (int): how often to update the network\n",
        "            replay_after (int): After which replay to be started\n",
        "            model(Model): Pytorch Model\n",
        "            base_filename (str): base filename to save models to\n",
        "        }\n",
        "        \"\"\"\n",
        "        self.input_shape = args['input_shape']\n",
        "        self.action_size = args['action_size']\n",
        "        self.device = args['device']\n",
        "        self.buffer_size = args['buffer_size']\n",
        "        self.batch_size = args['batch_size']\n",
        "        self.gamma = args['gamma']\n",
        "        self.lr = args['lr']\n",
        "        self.learn_every = args['learn_every']\n",
        "        self.replay_after = args['replay_after']\n",
        "        self.network = args['model']\n",
        "        self.tau = args['tau']\n",
        "        self.base_filename = args['base_filename']\n",
        "\n",
        "        \n",
        "        # Q-Network\n",
        "        self.policy_net = self.network(self.input_shape, self.action_size).to(self.device)\n",
        "        self.target_net = self.network(self.input_shape, self.action_size).to(self.device)\n",
        "        self.optimizer = torch.optim.AdamW(self.policy_net.parameters(), lr=self.lr, )\n",
        "        \n",
        "        self.memory = ReplayMemory(self.buffer_size, self.batch_size, self.device)\n",
        "        \n",
        "        self.time_step = 0\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.policy_net.state_dict(), f\"{self.base_filename}.policy.net\")\n",
        "        torch.save(self.target_net.state_dict(), f\"{self.base_filename}.target.net\")\n",
        "        self.memory.save(f\"{self.base_filename}.memory\")\n",
        "\n",
        "    def load(self):\n",
        "        self.memory = ReplayMemory(self.buffer_size, self.batch_size, self.device, f\"{self.base_filename}.memory\")\n",
        "        self.policy_net.load_state_dict(torch.load(f\"{self.base_filename}.policy.net\"))\n",
        "        self.target_net.load_state_dict(torch.load(f\"{self.base_filename}.target.net\"))\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        \n",
        "        # Learn every \"learn_every\" time steps.\n",
        "        self.time_step = (self.time_step + 1) % self.learn_every\n",
        "        if self.time_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > self.replay_after:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences)\n",
        "\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        return actFunc(self.policy_net, state, self.device, eps)\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        if torch.rand(1).item() < eps: # Epsilon-greedy action selection\n",
        "            return torch.randint(self.action_size, (1,)).item(), 2\n",
        "        \n",
        "        self.policy_net.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            state = torch.from_numpy(state).unsqueeze(0).to(self.device)\n",
        "            action_values = self.policy_net(state)\n",
        "            action_selection = action_values.detach().argmax().cpu().numpy()\n",
        "        self.policy_net.train() # set the model back to training mode\n",
        "        return action_selection, 1\n",
        "            \n",
        "    def learn(self, experiences): #input = 1 mini-batch\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Get expected Q values from the policy, and max predicted Q values from the target\n",
        "        Q_expected = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        targets_next = self.target_net(next_states).detach().max(dim=1)[0]\n",
        "        \n",
        "        # Calculate the Q value\n",
        "        # Multiply by (1 - done) to zero out the next state Q values if the game ended.\n",
        "        Q_targets = rewards + (self.gamma * targets_next * (1 - dones))\n",
        "        \n",
        "        # optimise the model, by minimising the loss\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.soft_update(self.policy_net, self.target_net, self.tau)\n",
        "\n",
        "    \n",
        "    # θ'=θ×τ+θ'×(1−τ)\n",
        "    def soft_update(self, policy_model, target_model, tau):\n",
        "        for target_param, policy_param in zip(target_model.parameters(), policy_model.parameters()):\n",
        "            target_param.data.copy_(tau*policy_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "\n",
        "# def actFunc(policy_net: nn.Module, state, device, eps=0.):\n",
        "#     if torch.rand(1).item() < eps: # Epsilon-greedy action selection\n",
        "#         return torch.randint(4, (1,)).item(), True\n",
        "    \n",
        "#     policy_net.eval() # set the model to evaluation mode\n",
        "#     with torch.no_grad():\n",
        "#         state = torch.from_numpy(state).unsqueeze(0).to(device)\n",
        "#         action_values = policy_net(state)\n",
        "#         action_selection = action_values.detach().argmax().cpu().numpy()\n",
        "#     policy_net.train() # set the model back to training mode\n",
        "#     return action_selection, False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj38QqcR2tw1"
      },
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IuCDHnnL2M8i"
      },
      "outputs": [],
      "source": [
        "def epsilon_decrease_func(start, end, decay):\n",
        "    def epsilon_decrease(i):\n",
        "        return end + (start - end) * math.exp(-1. * i / decay)\n",
        "    return epsilon_decrease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nNC_i9PE1TiX"
      },
      "outputs": [],
      "source": [
        "def trainDQN(agent, epsilon_decrease, n_episodes=1000, network='DQNcnn', start_epoch = 0, save_every=100, plot_every=500, log_eps=False):\n",
        "    print(f\"Training for {n_episodes} episodes, train every {agent.learn_every} episodes = {int(n_episodes/agent.learn_every)} train epoch\")\n",
        "    scores = []\n",
        "    stack_frames = get_stack_frames(network)\n",
        "    for i_episode in range(start_epoch + 1, start_epoch + n_episodes + 1):\n",
        "        state = stack_frames(None, env.reset(), is_new=True)\n",
        "        score, done = 0, False\n",
        "        eps = epsilon_decrease(i_episode)\n",
        "        if log_eps: actionTypes, actionTypesIdx = np.zeros(n_episodes*10, dtype=np.uint8), 0\n",
        "        while not done:\n",
        "            action, actionType = agent.act(state, eps)\n",
        "            if log_eps:\n",
        "                actionTypes[actionTypesIdx] = actionType\n",
        "                actionTypesIdx += 1\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            next_state = stack_frames(state, next_state, is_new=False)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "        scores.append(score)\n",
        "        log_eps_str = f'rand({round(np.count_nonzero(actionTypes[actionTypes == 2])/actionTypesIdx,2)}) nural({round(np.count_nonzero(actionTypes[actionTypes == 1])/actionTypesIdx,2)})    ' if log_eps else \"\"\n",
        "        print(f'\\rEpisode {i_episode}\\tScore: {scores[-1]}\\tAverage Score: {round(np.mean(scores[-20:]), 2)}\\t eps:{round(eps, 2)}\\t {log_eps_str}', end=\"\")\n",
        "        \n",
        "        if i_episode % plot_every == 0:\n",
        "            print(f'\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores[-50:])}')\n",
        "            fig = plt.figure()\n",
        "            ax = fig.add_subplot(111)\n",
        "            plt.plot(np.arange(len(scores)), scores)\n",
        "            plt.ylabel('Score')\n",
        "            plt.xlabel('Episode #')\n",
        "            plt.show()\n",
        "        if i_episode % save_every == 0:\n",
        "            agent.save()\n",
        "    agent.save()\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw-odSlqxgP8"
      },
      "source": [
        "### run train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Vxf8Qb6Nndf2"
      },
      "outputs": [],
      "source": [
        "model = 'ResNetDQN' # options are 'DQNcnn' or 'ResNetDQN'\n",
        "\n",
        "Number_of_episodes = 8000\n",
        "save_models_every = 250\n",
        "train_every = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx6Xu8kRndf2"
      },
      "source": [
        "#### training runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "8OuNTFsI3C_z",
        "outputId": "a3fdcd9e-8055-4e8c-c221-ae43098ed536"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "env = wrap_env(gym.make(env_name), record_every=1000)\n",
        "\n",
        "agent_args = {\n",
        "            'input_shape': (4, frame_crops[model], frame_crops[model]),\n",
        "            'action_size': env.action_space.n,\n",
        "            'device': device,\n",
        "            'buffer_size': 100000,\n",
        "            'batch_size': 64,\n",
        "            'gamma': 0.99,\n",
        "            'lr': 1e-4,\n",
        "            'tau': 1e-3,\n",
        "            'learn_every': train_every,\n",
        "            'replay_after': 10000,\n",
        "        }\n",
        "\n",
        "if model == 'DQNcnn':\n",
        "    agent_args = {**agent_args, 'model': DQNCnn, 'base_filename': 'atari_atlantas_models1'}\n",
        "elif model == 'ResNetDQN':\n",
        "    agent_args = {**agent_args, 'model': ResNetDQN, 'base_filename': 'atari_atlantas_resnet'}\n",
        "else:\n",
        "    raise Exception('model not found')\n",
        "\n",
        "epsilon_decrease = epsilon_decrease_func(start=0.3, end=0.02, decay=Number_of_episodes/8)\n",
        "# epsilon_decrease = lambda x: 0.000001\n",
        "plot_epsilon_func(epsilon_decrease, Number_of_episodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3DpdbkZndf3"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2XfmuL5rndf4",
        "outputId": "8b896933-73de-4b12-dd79-4485999216bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e12650432633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_decrease\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNumber_of_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_models_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-43d5d80dadd7>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplayMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{self.base_filename}.memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.base_filename}.policy.net\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.base_filename}.target.net\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-b1dea0f1a7ea>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, buffer_size, batch_size, device, filename, topHalf)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mReplayMemory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopHalf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtopHalf\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopHalf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopHalf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'atari_atlantas_resnet.memory'"
          ]
        }
      ],
      "source": [
        "agent = DQNAgent(agent_args)\n",
        "agent.load()\n",
        "scores = trainDQN(agent, epsilon_decrease, n_episodes=Number_of_episodes, save_every=save_models_every, plot_every=1000, network=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDIRbuQUndf8"
      },
      "outputs": [],
      "source": [
        "_ = plt.hist(scores, bins=30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzRetiqKndf-"
      },
      "outputs": [],
      "source": [
        "show_video()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE-yFm4M4DoR"
      },
      "outputs": [],
      "source": [
        "def finalPlay(agent):\n",
        "    env = wrap_env(gym.make(env_name))\n",
        "    score = 0\n",
        "    state = stack_frames(None, env.reset(), True)\n",
        "    while True:\n",
        "        # env.render()\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        score += reward\n",
        "        state = stack_frames(state, next_state)\n",
        "        if done:\n",
        "            print(\"You Final score is:\", score)\n",
        "            break \n",
        "    env.close()\n",
        "finalPlay(agent)\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyBB-sqS3F0i"
      },
      "source": [
        "### DQN2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeDcleB_WV2W"
      },
      "outputs": [],
      "source": [
        "class DQN2(nn.Module):\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Number of Linear input connections depends on output of conv2d layers\n",
        "        # and therefore the input image size, so compute it.\n",
        "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        print(\"in size\", x.size())\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        print(\"2 size\", x.size())\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        print(\"3 size\", x.size())\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_bgOQObndgD"
      },
      "outputs": [],
      "source": [
        "def get_output_size(in_size, kernel_size, stride=1, padding=0):\n",
        "    integer = int((in_size - kernel_size + 2 * padding) / stride) == ((in_size - kernel_size + 2 * padding) / stride)\n",
        "    if not integer:\n",
        "        print(\"not int\", ((in_size - kernel_size + 2 * padding) / stride))\n",
        "    return int((in_size - kernel_size + 2 * padding) / stride) + 1\n",
        "\n",
        "get_output_size(in_size=84, kernel_size=3, stride=2, padding=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqqu-qq39Swm"
      },
      "outputs": [],
      "source": [
        "class DQNtst(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super().__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        #in = 299x299\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=0), #out = 149x149\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            # nn.Dropout(0.3),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0), #out = 147x147\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), #out = 147x147\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0) #out = 73x73                    \n",
        "        )\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "                  nn.Linear(state_space_dim,64),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Linear(64,64*2),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Linear(64*2,action_space_dim)\n",
        "                )\n",
        "# (33600x3 and 210x64)\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        print('in size=', x.size())\n",
        "        x = self.block1(x)\n",
        "        print('block1 size=', x.size())\n",
        "        return self.linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRwZZtBSS4ti"
      },
      "outputs": [],
      "source": [
        "class DQNCnn(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super().__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "        \n",
        "        self.features = nn.Sequential( # in = 84x84\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        ) # out = 7x7\n",
        "        self.feature_size = 7 * 7 * 64\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, self.num_actions)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bfd5JgWoICcg",
        "zyBEYPP0H-qS",
        "yeaT5VbKKpSE"
      ],
      "name": "working.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (env)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}